{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pytorch = pd.read_csv(\"3_4_Data/pytorch_Qwen2-0.5B-Instruct.csv\")\n",
    "df_llama = pd.read_csv('3_4_Data/llamacpp_Qwen2-0.5B-Instruct-GGUF.csv')\n",
    "\n",
    "ram_dict = {\"PyTorch\": df_pytorch[\"max_ram\"].mean(), \"LLAMA\": df_llama[\"max_ram\"].mean()}\n",
    "cpu_dict = {\"PyTorch\": df_pytorch[\"cpu_usage\"].mean(), \"LLAMA\": df_llama[\"cpu_usage\"].mean()}\n",
    "latency_dict = {\"PyTorch\": df_pytorch[\"latency\"].mean(), \"LLAMA\": df_llama[\"latency\"].mean()}\n",
    "throughput_dict = {\"PyTorch\": df_pytorch[\"throughput\"].mean(), \"LLAMA\": df_llama[\"throughput\"].mean()}\n",
    "\n",
    "ram_dict, cpu_dict, latency_dict, throughput_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pytorch = pd.read_csv(\"3_4_Data/batch_size_benchmark_Qwen2-0.5B-Instruct.csv\")\n",
    "df_llama = pd.read_csv('3_4_Data/batch_size_benchmark_Qwen2-1.5B-Instruct.csv')\n",
    "\n",
    "ram_dict = {\"PyTorch\": df_pytorch[\"max_ram\"].mean(), \"LLAMA\": df_llama[\"max_ram\"].mean()}\n",
    "cpu_dict = {\"PyTorch\": df_pytorch[\"cpu_usage\"].mean(), \"LLAMA\": df_llama[\"cpu_usage\"].mean()}\n",
    "latency_dict = {\"PyTorch\": df_pytorch[\"latency\"].mean(), \"LLAMA\": df_llama[\"latency\"].mean()}\n",
    "throughput_dict = {\"PyTorch\": df_pytorch[\"throughput\"].mean(), \"LLAMA\": df_llama[\"throughput\"].mean()}\n",
    "\n",
    "ram_dict, cpu_dict, latency_dict, throughput_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plotting Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_dumbbell_charts(model_names, metrics, model_data, units=None):\n",
    "    \"\"\"\n",
    "    Creates a vertical panel of dumbbell charts, one per metric.\n",
    "    Each metric has an independent horizontal axis and two points:\n",
    "    one for each model, connected by a line.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_names: list of str, e.g. ['PyTorch', 'LLAMA']\n",
    "    - metrics: list of str, e.g. ['Max RAM', 'CPU Utilization', 'Latency', 'Throughput']\n",
    "    - model_data: dict of dict, where model_data[model][metric] = numeric value\n",
    "    - units: optional list of str, same length as metrics, e.g. ['Gb', '%', 's', 'Tokens/s']\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")  # cleaner grid style\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    \n",
    "    # Create a vertical set of subplots (1 column, n_metrics rows)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_metrics, ncols=1, \n",
    "        figsize=(8, 2.5 * n_metrics),  # Adjust height as needed\n",
    "        dpi=350\n",
    "    )\n",
    "\n",
    "    # Ensure axes is always iterable (even if n_metrics=1)\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Retrieve data for both models\n",
    "        py_val = model_data[model_names[0]][metric]\n",
    "        llama_val = model_data[model_names[1]][metric]\n",
    "        \n",
    "        # Build a small list of the two points\n",
    "        x_values = [py_val, llama_val]\n",
    "        \n",
    "        # For x-axis label, incorporate unit if available\n",
    "        if units and i < len(units) and units[i]:\n",
    "            metric_label = f\"{metric} ({units[i]})\"\n",
    "        else:\n",
    "            metric_label = metric\n",
    "        \n",
    "        # We'll place them on a single y-level, e.g., y=0\n",
    "        # Then draw a line between them\n",
    "        ax.plot(\n",
    "            x_values, [0, 0], \n",
    "            color=\"gray\", linewidth=2, \n",
    "            zorder=1\n",
    "        )\n",
    "        \n",
    "        # Plot each point separately with distinct colors\n",
    "        ax.scatter(\n",
    "            py_val, 0, \n",
    "            color=\"#66b3ff\", edgecolor=\"black\", s=100, \n",
    "            label=model_names[0], zorder=2\n",
    "        )\n",
    "        ax.scatter(\n",
    "            llama_val, 0, \n",
    "            color=\"#ff9999\", edgecolor=\"black\", s=100, \n",
    "            label=model_names[1], zorder=2\n",
    "        )\n",
    "        \n",
    "        # Annotate each point with its numeric value\n",
    "        # Slight horizontal offset so text doesn't collide\n",
    "        ax.text(\n",
    "            py_val, 0.05, \n",
    "            f\"{py_val:.2f}\", \n",
    "            ha=\"center\", va=\"bottom\", \n",
    "            fontsize=10\n",
    "        )\n",
    "        ax.text(\n",
    "            llama_val, -0.05, \n",
    "            f\"{llama_val:.2f}\", \n",
    "            ha=\"center\", va=\"top\", \n",
    "            fontsize=10\n",
    "        )\n",
    "        \n",
    "        # Set title on the left side (or top). We'll just use set_title for clarity.\n",
    "        ax.set_title(metric_label, fontsize=12, loc=\"left\", pad=10)\n",
    "        \n",
    "        # Hide the y-axis, since we only care about the horizontal scale\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylabel(\"\")\n",
    "        \n",
    "        # Optionally show a minimal x-axis\n",
    "        ax.tick_params(axis=\"x\", labelsize=10)\n",
    "        \n",
    "        # If you want a legend, you can create one on the last subplot or overall\n",
    "        if i == 0:  # put a legend on the first subplot\n",
    "            ax.legend(loc=\"upper left\", frameon=False, fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model_data = {\n",
    "    \"PyTorch\": {\n",
    "        \"Max RAM\": ram_dict[\"PyTorch\"],\n",
    "        \"CPU Utilization\": cpu_dict[\"PyTorch\"],\n",
    "        \"Latency\": latency_dict[\"PyTorch\"],\n",
    "        \"Throughput\": throughput_dict[\"PyTorch\"]\n",
    "    },\n",
    "    \"LLAMA\": {\n",
    "        \"Max RAM\": ram_dict[\"LLAMA\"],\n",
    "        \"CPU Utilization\": cpu_dict[\"LLAMA\"],\n",
    "        \"Latency\": latency_dict[\"LLAMA\"],\n",
    "        \"Throughput\": throughput_dict[\"LLAMA\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "model_names = [\"PyTorch\", \"LLAMA\"]\n",
    "metrics = [\"Max RAM\", \"CPU Utilization\", \"Latency\", \"Throughput\"]\n",
    "units = [\"Gb\", \"%\", \"s\", \"Tokens/s\"]  # Provide units if necessary\n",
    "\n",
    "# Plot the 2x2 grid of metrics\n",
    "plot_dumbbell_charts(model_names, metrics, model_data, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_model_names = [\n",
    "    'Qwen2 0.5B',\n",
    "    'Llama 3.2 1B',\n",
    "    'R1-Distill-Qwen 1.5B',\n",
    "    'Qwen2 1.5B',\n",
    "    'Llama 3.2 3B',\n",
    "    'R1-Distill-Qwen 7B',\n",
    "    'Qwen2 7B',\n",
    "    'Llama 3.1 8B',\n",
    "    'R1-Distill-Llama 8B'\n",
    "]\n",
    "\n",
    "metrics = ['Accuracy', 'Latency', 'Throughput', 'Max RAM', 'Tokens Generated']\n",
    "units = [\"\", \"s\", \"tokens/s\", \"GB\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_metrics(model_names, metrics, units, model_data):\n",
    "    \"\"\"\n",
    "    Plots bar graphs for each metric across the provided model names.\n",
    "\n",
    "    Parameters:\n",
    "    - model_names: list of str, the names of the models to include.\n",
    "    - metrics: list of str, the metrics to plot (e.g., 'Latency', 'Throughput').\n",
    "    - units: list of str, the corresponding units for each metric.\n",
    "    - model_data: dict, where keys are model names and values are dictionaries \n",
    "                  containing the metric values.\n",
    "    \"\"\"\n",
    "    \n",
    "    for metric, unit in zip(metrics, units):\n",
    "        # Extract metric values for the models (ignore models missing the metric)\n",
    "        unit_str = f\" ({unit})\" if unit else \"\"\n",
    "        values = []\n",
    "        selected_models = []\n",
    "        for model in model_names:\n",
    "            if model in model_data and metric in model_data[model]:\n",
    "                values.append(model_data[model][metric])\n",
    "                selected_models.append(model)\n",
    "\n",
    "        # Create a new figure for each metric with 350 dpi for publication quality\n",
    "        plt.figure(figsize=(8, 5), dpi=350)\n",
    "\n",
    "        # Plot bars with a custom color, bar width, and edge color\n",
    "        bars = plt.bar(\n",
    "            selected_models, \n",
    "            values, \n",
    "            color='#66b3ff',   # Light blue\n",
    "            width=0.6         # Adjust as needed for thickness\n",
    "        )\n",
    "        \n",
    "        # Add a title and axis labels with increased font sizes\n",
    "        plt.title(f\"{metric}{unit_str}\", fontsize=16, pad=10)\n",
    "        # plt.xlabel(\"Models\", fontsize=14)\n",
    "        plt.ylabel(f\"{metric}{unit_str}\", fontsize=14)\n",
    "\n",
    "        # Rotate x-axis labels for clarity with increased font size\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "\n",
    "        # Add numeric labels on top of each bar with increased font size\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2,  # X position in center of bar\n",
    "                height,                            # Y position just above the bar\n",
    "                f\"{height:.2f}\",                   # Format to 2 decimal places\n",
    "                ha='center', va='bottom', \n",
    "                fontsize=12\n",
    "            )\n",
    "        \n",
    "        # Make layout fit nicely\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    'Qwen2 1.5B',\n",
    "    'R1-Distill-Qwen 1.5B',\n",
    "    'Qwen2 7B',\n",
    "    'R1-Distill-Qwen 7B',\n",
    "    'Llama 3.1 8B',\n",
    "    'R1-Distill-Llama 8B'\n",
    "]\n",
    "\n",
    "metrics = ['Accuracy', 'Latency', 'Throughput', 'Max RAM', 'Tokens Generated']\n",
    "units = [\"\", \"s\", \"tokens/s\", \"GB\", \"\"]\n",
    "plot_model_metrics(model_names, metrics, units, final_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'Qwen2 0.5B',\n",
    "    'Llama 3.2 1B',\n",
    "    'Qwen2 1.5B',\n",
    "    'Llama 3.2 3B',\n",
    "    'Qwen2 7B',\n",
    "    'Llama 3.1 8B',\n",
    "]\n",
    "\n",
    "metrics = ['Accuracy', 'Latency', 'Throughput', 'Max RAM']\n",
    "units = [\"\", \"s\", \"tokens/s\", \"GB\"]\n",
    "plot_model_metrics(model_names, metrics, units, final_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Time Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_position(model_output_whole, answer_key):\n",
    "    \"\"\"\n",
    "    Evaluates the model output by scanning the entire string for the answer_key.\n",
    "    \n",
    "    For numeric answer_keys, it searches for any number (allowing an optional '$' and commas)\n",
    "    that, when converted to a float, equals answer_key. For non-numeric answer_keys, it checks\n",
    "    whether the answer_key appears anywhere in the string.\n",
    "    \n",
    "    Returns:\n",
    "        1 if a matching answer is found, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Determine whether answer_key is numeric.\n",
    "    model_output_list = model_output_whole.split(\" \")\n",
    "    for index1 in range(len(model_output_list)):\n",
    "        if answer_key in model_output_list[index1]:\n",
    "            return [index1, len(model_output_list)]\n",
    "    return [float('inf'), len(model_output_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_model_outputs_to_study = ['DeepSeek-R1-Distill-Llama-8B-Q4_K_M_gguf.txt',\n",
    " 'DeepSeek-R1-Distill-Qwen-1_5B-Q4_K_M_gguf.txt',\n",
    " 'DeepSeek-R1-Distill-Qwen-7B-Q6_K_gguf.txt',\n",
    " 'Llama-3_2-1B-Instruct_Q4_K_M_gguf.txt',\n",
    " 'Llama-3_2-3B-Instruct-Q4_K_M_gguf.txt',\n",
    " 'Meta-Llama-3_1-8B-Instruct-Q4_K_M_gguf.txt',\n",
    " 'qwen2-0_5b-instruct-q4_k_m_gguf.txt',\n",
    " 'qwen2-1_5b-instruct-q4_k_m_gguf.txt',\n",
    " 'qwen2-7b-instruct-q4_k_m_gguf.txt']\n",
    "\n",
    "file_name_to_model_name = {\n",
    "    'Llama-3_2-1B-Instruct_IQ1_M_gguf.txt': 'Llama 3.2 1B 1-bit',\n",
    "    'Llama-3_2-1B-Instruct_Q2_K_gguf.txt': 'Llama 3.2 1B 2-bit',\n",
    "    'Llama-3_2-1B-Instruct_Q4_K_M_gguf.txt': 'Llama 3.2 1B 4-bit',\n",
    "    'Llama-3_2-1B-Instruct_Q8_0_gguf.txt': 'Llama 3.2 1B 8-bit',\n",
    "    'Llama-3_2-3B-Instruct_IQ1_M_gguf.txt': 'Llama 3.2 3B 1-bit',\n",
    "    'Llama-3_2-3B-Instruct_Q2_K_gguf.txt': 'Llama 3.2 3B 2-bit',\n",
    "    'Llama-3_2-3B-Instruct_Q4_K_M_gguf.txt': 'Llama 3.2 3B 4-bit',\n",
    "    'Llama-3_2-3B-Instruct_Q8_0_gguf.txt': 'Llama 3.2 3B 8-bit',\n",
    "    'Meta-Llama-3-8B-Instruct_IQ1_M_gguf.txt': 'Llama 3.0 8B 1-bit',\n",
    "    'Meta-Llama-3-8B-Instruct_IQ2_XS_gguf.txt': 'Llama 3.0 8B 2-bit',\n",
    "    'Meta-Llama-3-8B-Instruct_Q4_K_S_gguf.txt': 'Llama 3.0 8B 4-bit',\n",
    "    'Meta-Llama-3-8B-Instruct_Q8_0_gguf.txt': 'Llama 3.0 8B 8-bit'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_outputs(model_name):\n",
    "    \"\"\"\n",
    "    Reads the output file for the given model and returns a list of dictionaries,\n",
    "    each representing the results for one prompt.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The original model name (e.g., \"some/model\").\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries read from the file.\n",
    "    \"\"\"\n",
    "    # Sanitize the model name (replace \"/\" with \"_\") to match the file naming\n",
    "    model_name_sanitized = model_name.replace(\"/\", \"_\")\n",
    "    file_path = f\"model_outputs_quant/{model_name_sanitized}\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"No output file found for model '{model_name_sanitized}'.\")\n",
    "        print(f\"No output file found for model '{model_name}'.\")\n",
    "        return results\n",
    "    \n",
    "    # Open and read the file line by line\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    # Convert the string representation of the dictionary into a dict\n",
    "                    result_dict = ast.literal_eval(line)\n",
    "                    results.append(result_dict)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing line: {line}\\nError: {e}\")\n",
    "                    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_results = {}\n",
    "for model_index in range(len(list_of_model_outputs_to_study)):\n",
    "    data_out = read_model_outputs(list_of_model_outputs_to_study[model_index])\n",
    "    print(f\"Model: {file_name_to_model_name[list_of_model_outputs_to_study[model_index]]}\")\n",
    "    model_results = []\n",
    "    answer_positions = []\n",
    "    correct_answers = 0\n",
    "    count = 0\n",
    "    for i in tqdm(range(int(len(data_out)))):\n",
    "        model_output = data_out[i]['model_output'] \n",
    "        answer_key = data_out[i]['answer']\n",
    "        values_found = answer_position(model_output, \"{:g}\".format(answer_key))\n",
    "        if values_found[0] != float('inf'):\n",
    "            correct_answers += 1\n",
    "        latency = data_out[i]['latency'] \n",
    "        throughput = data_out[i]['throughput']\n",
    "        time_to_answer = values_found[0]*latency/values_found[1] \n",
    "        count += 1\n",
    "        model_results = model_results + [time_to_answer]\n",
    "        answer_positions = answer_positions + [values_found[0]]\n",
    "    final_model_results[file_name_to_model_name[list_of_model_outputs_to_study[model_index]]] = {\"model_results\": model_results, \"accuracy\" : correct_answers/count, \"answer_positions\": answer_positions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_cdfs_line_chart(datasets, labels=None, figsize=(8, 5), max_x=None):\n",
    "    \"\"\"\n",
    "    Plot the CDFs for multiple 'time_to_answers' datasets as a line chart.\n",
    "    \n",
    "    For each dataset, the CDF is computed as the fraction of all values (including infinities)\n",
    "    that are less than or equal to a given value. The plot is drawn only over the finite portion\n",
    "    of the data (up to the dataset's maximum finite value) and then extended horizontally to a \n",
    "    specified maximum x value (either provided via 'max_x' or determined as the global maximum \n",
    "    finite value across all datasets). This clearly indicates that the CDF does not reach 1 \n",
    "    when there are infinite values.\n",
    "    \n",
    "    Parameters:\n",
    "        datasets (list of lists/arrays): Each element is a dataset of numbers.\n",
    "        labels (list of str, optional): Labels for the datasets. If not provided, default names \n",
    "            'Dataset 1', 'Dataset 2', etc. will be used.\n",
    "        figsize (tuple, optional): Figure size (default is (8, 5)).\n",
    "        max_x (float, optional): The maximum x value to plot. If provided, each dataset's line will \n",
    "            be extended horizontally to this value. If not provided, the maximum finite value across \n",
    "            all datasets is used.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new figure with publication quality dpi\n",
    "    plt.figure(figsize=figsize, dpi=350)\n",
    "    \n",
    "    # Compute the global maximum finite value across all datasets.\n",
    "    global_max_finite = None\n",
    "    for d in datasets:\n",
    "        d_arr = np.array(d)\n",
    "        finite_data = d_arr[np.isfinite(d_arr)]\n",
    "        if finite_data.size == 0:\n",
    "            raise ValueError(\"At least one dataset has all infinite values.\")\n",
    "        d_max = np.max(finite_data)\n",
    "        if global_max_finite is None or d_max > global_max_finite:\n",
    "            global_max_finite = d_max\n",
    "            \n",
    "    # Determine the x-axis maximum for plotting.\n",
    "    plot_max = max_x if max_x is not None else global_max_finite\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [f\"Dataset {i+1}\" for i in range(len(datasets))]\n",
    "    \n",
    "    for data, label in zip(datasets, labels):\n",
    "        data = np.array(data)\n",
    "        n_total = len(data)\n",
    "        \n",
    "        # Sort the entire dataset (infinities will appear at the end)\n",
    "        sorted_data = np.sort(data)\n",
    "        \n",
    "        # Compute the CDF including all points (infs included, so CDF may not reach 1)\n",
    "        cdf = np.arange(1, n_total + 1) / n_total\n",
    "        \n",
    "        # Identify the finite portion (all values <= dataset's maximum finite value)\n",
    "        finite_data = data[np.isfinite(data)]\n",
    "        dataset_max_finite = np.max(finite_data)\n",
    "        mask = sorted_data <= dataset_max_finite\n",
    "        \n",
    "        # Extract the finite portion to plot\n",
    "        x_plot = sorted_data[mask]\n",
    "        y_plot = cdf[mask]\n",
    "        \n",
    "        # Extend the line horizontally to the plot_max value if needed.\n",
    "        if dataset_max_finite < plot_max:\n",
    "            x_plot = np.append(x_plot, plot_max)\n",
    "            y_plot = np.append(y_plot, y_plot[-1])\n",
    "        \n",
    "        # Plot as a line chart with markers at the data points.\n",
    "        plt.plot(x_plot, y_plot, label=label, marker='o')\n",
    "    \n",
    "    # Set labels and title with updated font sizes.\n",
    "    plt.xlabel('Time (in seconds)', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.title('Accuracy vs Inference Time', fontsize=16, pad=10)\n",
    "    \n",
    "    # Set tick font sizes\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, plot_max)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_plot = [\n",
    " 'Llama 3.2 1B',\n",
    " 'Llama 3.2 3B',\n",
    " 'Llama 3.1 8B',\n",
    " ]\n",
    "\n",
    "list_of_datasets = []\n",
    "list_of_labels = []\n",
    "for key1 in models_to_plot:\n",
    "    list_of_datasets.append(final_model_results[key1][\"model_results\"])\n",
    "    list_of_labels.append(key1)\n",
    "plot_multiple_cdfs_line_chart(list_of_datasets, list_of_labels, figsize=(8, 5), max_x = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_plot = [\n",
    " 'Qwen2 0.5B',\n",
    " 'Qwen2 1.5B',\n",
    " 'Qwen2 7B']\n",
    "\n",
    "list_of_datasets = []\n",
    "list_of_labels = []\n",
    "for key1 in models_to_plot:\n",
    "    list_of_datasets.append(final_model_results[key1][\"model_results\"])\n",
    "    list_of_labels.append(key1)\n",
    "plot_multiple_cdfs_line_chart(list_of_datasets, list_of_labels, figsize=(8, 5), max_x = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_plot = ['R1-Distill-Llama 8B',\n",
    "'R1-Distill-Qwen 7B',\n",
    "'Llama 3.1 8B',\n",
    "'Qwen2 7B'\n",
    "]\n",
    "\n",
    "list_of_datasets = []\n",
    "list_of_labels = []\n",
    "for key1 in models_to_plot:\n",
    "    list_of_datasets.append(final_model_results[key1][\"model_results\"])\n",
    "    list_of_labels.append(key1)\n",
    "plot_multiple_cdfs_line_chart(list_of_datasets, list_of_labels, figsize=(8, 5), max_x = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
