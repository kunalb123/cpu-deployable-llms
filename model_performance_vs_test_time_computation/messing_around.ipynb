{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    '/Users/kunalbhandarkar/Downloads/Llama-3.2-1B-Instruct.Q5_K_S.gguf',           # smaller llama model\n",
    "    '/Users/kunalbhandarkar/Downloads/Llama-3.2-1B-Instruct.Q8_0.gguf',             # larger llama model\n",
    "    '/Users/kunalbhandarkar/Downloads/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf',     # smaller supposedly better model\n",
    "    '/Users/kunalbhandarkar/Downloads/DeepSeek-R1-Distill-Qwen-1.5B-f32.gguf',      # 3 GB model\n",
    "    '/Users/kunalbhandarkar/Downloads/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf',     # larger size model\n",
    "    '/Users/kunalbhandarkar/Downloads/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1319 prompts from new_prompts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_prompts(file_path):\n",
    "    \"\"\"\n",
    "    Loads the list of prompt strings from a JSON file.\n",
    "   \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file containing the prompt strings.\n",
    "   \n",
    "    Returns:\n",
    "        list: The list of prompt strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        prompts = json.load(f)\n",
    "    return prompts\n",
    "\n",
    "output_file = 'new_prompts.json'\n",
    "# Load back the list to verify\n",
    "loaded_prompts = load_prompts(output_file)\n",
    "print(f\"Loaded {len(loaded_prompts)} prompts from {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "### Response: Let's think step by step.\n"
     ]
    }
   ],
   "source": [
    "print(loaded_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This class takes in the path to a gguf file representing an LLM\n",
    "and allows a user to interact with the LLM running on a CPU.\n",
    "\n",
    "'''\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "class CPUModel:\n",
    "\n",
    "    # pass in the path to the gguf file\n",
    "    def __init__(self, model_path):\n",
    "        self.llm = Llama(model_path=model_path)\n",
    "\n",
    "    # returns the complete response (a dictionary) of the LLM given an input\n",
    "    # args is a dict containing input_text: string that is the input to the LLM\n",
    "    # as well as other optional model parameters like temperature (float),\n",
    "    # max_tokens (int), and top_p (float)\n",
    "    def get_response(self, args: dict):\n",
    "        return self.llm(\n",
    "            args['input_text'],\n",
    "            max_tokens=args.get('max_tokens'),\n",
    "            temperature=args.get('temperature'),\n",
    "            top_p=args.get('top_p')\n",
    "        )\n",
    "\n",
    "    # returns the first \"choice\" that the model outputs. this is just the text\n",
    "    # that the model outputs without the other information on the response\n",
    "    # args is a dict containing input_text: string that is the input to the LLM\n",
    "    # as well as other optional model parameters like temperature (float),\n",
    "    # max_tokens (int), and top_p (float)\n",
    "    def get_text_response(self, args: dict):\n",
    "        response = self.get_response(args)\n",
    "        return response['choices'][0]['text']\n",
    "\n",
    "    # to be implemented\n",
    "    def evaluate(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Pro) - 12282 MiB free\n",
      "llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors from /Users/kunalbhandarkar/Downloads/Llama-3.2-1B-Instruct.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models Meta Llama Llama 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = models-meta-llama-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = ./Llama-3.2-1B-Instruct-GGUF_imatrix.dat\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 112\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 68\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q5_K:  112 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Small\n",
      "print_info: file size   = 843.75 MiB (5.73 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = Models Meta Llama Llama 3.2 1B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 128 'Ä'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 162 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/17 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =   843.75 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 500000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Pro\n",
      "ggml_metal_init: picking default device: Apple M3 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12884.92 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x14fb452c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x14f2707e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x10b03fe20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x10b03f570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x10b040b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x10b216110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x10b040f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x14f270df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x10b041cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x10b042520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x10b216910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x10b042910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x14f271920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x10b214fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x10b0413c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x10b0433a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x14f2711c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x10b216d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x109f0c4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x14f2727b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x10b217a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f272f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x149b7d2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x149b7dfd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x109f0cf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x109f0d560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x109f0d9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x10b2181d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10b218650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f274370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f2735c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x10b043a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x10b0446b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109f2f750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10b219250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10b21a180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109f30360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10b21a500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10b21ada0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10b21bee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10b21b640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10b21c760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149b7ea20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109f30be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10b044c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109f2fac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f2748b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149b7f270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10b046a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f275200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109f31c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10b046d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x149b7fe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x149b80cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x109f32c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x10b0472e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10b21d000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149b80550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149b7e600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109f32ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10b21d570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10b0476d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109f33420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10b21f160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f275910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10b21fa20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10b220280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f276ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f277340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149b82240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x109f34090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10b048220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149b82b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109f349b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109f35270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10b21f420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10b048b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10b221000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149b81370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10b221900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10b047b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10b049970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10b04a240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109f338d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149b83350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149b84260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10b220910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10b222bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f276f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f277ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f277e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f278310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149b83bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f278aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149b84d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a910420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12903e2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10b222360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128a97860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x149b85aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f278d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f2791d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10b0492b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10b223360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10b04b210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10b04bdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128a9ec80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f27aa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a9106e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109f365b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128a934a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149b85270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10b04c7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10b224220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10b223620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10b223a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10b04d040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10b224ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10b225ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f279880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109f36f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10b04dc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10b225260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149b86670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10b04b6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109f37da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149b87570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149b87e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f27c090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10b2262d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f27c740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f27cb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149b88da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10b226660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109f384f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149b86f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149b89530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x149b89970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149b89cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149b8b670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f27cfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10b226e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10b2275d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10b228b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x149b8be40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10b04e3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10b04f6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10b04ff20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10b050390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10b229700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149b8c210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149b8d010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10b050c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10b229040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109f387b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10b050690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x109f39090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10b052080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10b22a400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f27e3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10b229cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10b22b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10b051890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10b22bd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10b053190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f27de70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10b053ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10b22acb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109f398a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10b052980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10b22cbf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10b22d550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f27fc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f2807f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149b8d850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10b22de60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f280190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149b8e1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f281680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f281fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f2828d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149b8eaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149b8c9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149b8fc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149b90560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x10b22c5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x149b8f370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x149b912d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x149b91700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x109f3ab80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x10b054200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109f3ae40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10b22eb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109f3c2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x109e0b6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x10b0549f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1290457b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128ab5680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128ab5e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x10b054fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10b22ee50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10b22fe00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10b230580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149b91b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149b929b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149b932d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f283420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10b10d470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a910ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f283ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149b924a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149b94650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10b055290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10b055d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10b231260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109f3ce90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109f3d9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109f3d380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f284310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109f3e2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10b055a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149b941b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149b95840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f2857a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10b231820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f286000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109f3e770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10b057500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10b056be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109f3ef90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14fae6a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f2868c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f286390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f286d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f287a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f287610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14fae6fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x149b93a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14fae7470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f2888e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x109f3f950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10b233250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149b96230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14fae7ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14fae8ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x109f40c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10b233b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149b97670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14fae7fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14faea0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10b233f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x14f289110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x14faea970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10b234860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14faead40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109f3f320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14faeb9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14faec1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149b97db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149b96900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149b98e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109f419b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109f42250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x109f41570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x109f42e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x14fae85f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x149b99290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x10b234c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x10b235240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x10b235bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f28a0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f28af30 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 16, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =    16.00 MiB\n",
      "llama_init_from_model: KV self size  =   16.00 MiB, K (f16):    8.00 MiB, V (f16):    8.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   254.50 MiB\n",
      "llama_init_from_model: graph nodes  = 518\n",
      "llama_init_from_model: graph splits = 258 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': './Llama-3.2-1B-Instruct-GGUF_imatrix.dat', 'quantize.imatrix.chunks_count': '68', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '2048', 'llama.vocab_size': '128256', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.attention.value_length': '64', 'llama.attention.head_count': '32', 'llama.attention.key_length': '64', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '16', 'llama.block_count': '16', 'general.size_label': '1B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '8192', 'general.quantization_version': '2', 'llama.rope.dimension_count': '64', 'general.license': 'llama3.2', 'quantize.imatrix.entries_count': '112', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'models-meta-llama-Llama-3.2', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': 'group_40.txt', 'general.name': 'Models Meta Llama Llama 3.2 1B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "cpu_model = CPUModel(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cpu_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTell me a knock knock joke\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_model\u001b[49m\u001b[38;5;241m.\u001b[39mget_text_response(args)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRESPONSE:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cpu_model' is not defined"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'temperature': 0.5,\n",
    "    'top_p': 0.3,\n",
    "    'max_tokens': 100\n",
    "}\n",
    "args['input_text'] = 'Tell me a knock knock joke'\n",
    "\n",
    "response = cpu_model.get_text_response(args)\n",
    "print('RESPONSE:')\n",
    "print(response)\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 22 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =    2945.16 ms /    75 tokens (   39.27 ms per token,    25.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1205.49 ms /    99 runs   (   12.18 ms per token,    82.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    2893.21 ms /   174 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 36 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     354.44 ms /    36 tokens (    9.85 ms per token,   101.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1202.91 ms /    99 runs   (   12.15 ms per token,    82.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    1576.98 ms /   135 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     336.36 ms /    59 tokens (    5.70 ms per token,   175.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1335.01 ms /    99 runs   (   13.48 ms per token,    74.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    1691.03 ms /   158 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 44 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     417.03 ms /    44 tokens (    9.48 ms per token,   105.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1191.13 ms /    99 runs   (   12.03 ms per token,    83.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1627.04 ms /   143 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 118 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     396.94 ms /   118 tokens (    3.36 ms per token,   297.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1283.92 ms /    99 runs   (   12.97 ms per token,    77.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    1700.98 ms /   217 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 64 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     477.74 ms /    64 tokens (    7.46 ms per token,   133.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.57 ms /    99 runs   (   12.50 ms per token,    80.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    1734.65 ms /   163 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     691.84 ms /    51 tokens (   13.57 ms per token,    73.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1730.24 ms /    99 runs   (   17.48 ms per token,    57.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    2443.33 ms /   150 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     769.40 ms /    76 tokens (   10.12 ms per token,    98.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1300.88 ms /    99 runs   (   13.14 ms per token,    76.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    2090.32 ms /   175 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 111 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     900.86 ms /   111 tokens (    8.12 ms per token,   123.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1242.88 ms /    99 runs   (   12.55 ms per token,    79.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    2163.43 ms /   210 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 67 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     342.52 ms /    67 tokens (    5.11 ms per token,   195.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1291.80 ms /    99 runs   (   13.05 ms per token,    76.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    1653.74 ms /   166 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     589.99 ms /    68 tokens (    8.68 ms per token,   115.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1244.91 ms /    99 runs   (   12.57 ms per token,    79.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1854.58 ms /   167 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     379.19 ms /    72 tokens (    5.27 ms per token,   189.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1228.96 ms /    99 runs   (   12.41 ms per token,    80.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    1627.43 ms /   171 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     373.22 ms /    76 tokens (    4.91 ms per token,   203.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1251.94 ms /    99 runs   (   12.65 ms per token,    79.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    1645.62 ms /   175 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     515.00 ms /    68 tokens (    7.57 ms per token,   132.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1197.47 ms /    99 runs   (   12.10 ms per token,    82.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1731.74 ms /   167 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     325.87 ms /    59 tokens (    5.52 ms per token,   181.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1154.67 ms /    99 runs   (   11.66 ms per token,    85.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.34 ms /   158 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 100 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     792.07 ms /   100 tokens (    7.92 ms per token,   126.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1324.52 ms /    99 runs   (   13.38 ms per token,    74.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    2137.58 ms /   199 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     333.35 ms /    60 tokens (    5.56 ms per token,   179.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.76 ms /    99 runs   (   11.22 ms per token,    89.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1462.26 ms /   159 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 63 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     430.09 ms /    63 tokens (    6.83 ms per token,   146.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1254.28 ms /    99 runs   (   12.67 ms per token,    78.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1703.85 ms /   162 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     305.71 ms /    39 tokens (    7.84 ms per token,   127.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1292.55 ms /    99 runs   (   13.06 ms per token,    76.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    1617.96 ms /   138 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 74 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     663.26 ms /    74 tokens (    8.96 ms per token,   111.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1265.02 ms /    99 runs   (   12.78 ms per token,    78.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1948.40 ms /   173 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     509.93 ms /    70 tokens (    7.28 ms per token,   137.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.44 ms /    99 runs   (   11.60 ms per token,    86.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1677.30 ms /   169 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 54 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     601.87 ms /    54 tokens (   11.15 ms per token,    89.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1158.39 ms /    99 runs   (   11.70 ms per token,    85.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    1779.35 ms /   153 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     411.56 ms /    65 tokens (    6.33 ms per token,   157.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1150.56 ms /    99 runs   (   11.62 ms per token,    86.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    1580.66 ms /   164 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 48 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     306.77 ms /    48 tokens (    6.39 ms per token,   156.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1218.31 ms /    99 runs   (   12.31 ms per token,    81.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1544.22 ms /   147 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     335.47 ms /    46 tokens (    7.29 ms per token,   137.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.76 ms /    99 runs   (   11.36 ms per token,    88.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    1478.56 ms /   145 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 74 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     834.60 ms /    74 tokens (   11.28 ms per token,    88.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.29 ms /    99 runs   (   11.25 ms per token,    88.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1966.20 ms /   173 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 74 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     363.82 ms /    74 tokens (    4.92 ms per token,   203.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.85 ms /    99 runs   (   11.25 ms per token,    88.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.78 ms /   173 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 66 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     371.63 ms /    66 tokens (    5.63 ms per token,   177.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1152.44 ms /    99 runs   (   11.64 ms per token,    85.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    1542.45 ms /   165 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 57 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     741.80 ms /    57 tokens (   13.01 ms per token,    76.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1163.94 ms /    99 runs   (   11.76 ms per token,    85.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1924.70 ms /   156 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     517.49 ms /    78 tokens (    6.63 ms per token,   150.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1179.35 ms /    99 runs   (   11.91 ms per token,    83.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    1715.76 ms /   177 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 45 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     355.17 ms /    45 tokens (    7.89 ms per token,   126.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1176.45 ms /    99 runs   (   11.88 ms per token,    84.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1550.83 ms /   144 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     731.23 ms /    70 tokens (   10.45 ms per token,    95.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.75 ms /    99 runs   (   11.99 ms per token,    83.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1936.93 ms /   169 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     406.67 ms /    50 tokens (    8.13 ms per token,   122.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.58 ms /    99 runs   (   12.27 ms per token,    81.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    1640.61 ms /   149 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     339.29 ms /    38 tokens (    8.93 ms per token,   112.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1174.25 ms /    99 runs   (   11.86 ms per token,    84.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1532.21 ms /   137 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     370.86 ms /    50 tokens (    7.42 ms per token,   134.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.24 ms /    99 runs   (   11.58 ms per token,    86.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1535.75 ms /   149 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 58 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     302.47 ms /    58 tokens (    5.22 ms per token,   191.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1177.88 ms /    99 runs   (   11.90 ms per token,    84.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    1500.07 ms /   157 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 52 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     509.69 ms /    52 tokens (    9.80 ms per token,   102.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1166.18 ms /    99 runs   (   11.78 ms per token,    84.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    1694.77 ms /   151 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     491.06 ms /    70 tokens (    7.02 ms per token,   142.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1214.45 ms /    99 runs   (   12.27 ms per token,    81.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1724.45 ms /   169 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 52 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     357.17 ms /    52 tokens (    6.87 ms per token,   145.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1145.17 ms /    99 runs   (   11.57 ms per token,    86.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    1520.80 ms /   151 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 83 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     576.45 ms /    83 tokens (    6.95 ms per token,   143.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1193.24 ms /    99 runs   (   12.05 ms per token,    82.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1789.31 ms /   182 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 54 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     665.47 ms /    54 tokens (   12.32 ms per token,    81.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     998.95 ms /    89 runs   (   11.22 ms per token,    89.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1680.40 ms /   143 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 134 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     430.69 ms /   134 tokens (    3.21 ms per token,   311.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1165.14 ms /    99 runs   (   11.77 ms per token,    84.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    1615.24 ms /   233 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 91 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     439.99 ms /    91 tokens (    4.84 ms per token,   206.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1125.62 ms /    99 runs   (   11.37 ms per token,    87.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    1583.99 ms /   190 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 63 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     444.16 ms /    63 tokens (    7.05 ms per token,   141.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1187.24 ms /    99 runs   (   11.99 ms per token,    83.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1650.50 ms /   162 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     503.75 ms /    82 tokens (    6.14 ms per token,   162.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1164.28 ms /    99 runs   (   11.76 ms per token,    85.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1686.89 ms /   181 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 104 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     787.07 ms /   104 tokens (    7.57 ms per token,   132.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1223.14 ms /    99 runs   (   12.35 ms per token,    80.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    2030.09 ms /   203 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 100 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     450.52 ms /   100 tokens (    4.51 ms per token,   221.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1134.40 ms /    99 runs   (   11.46 ms per token,    87.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    1603.91 ms /   199 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 56 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     416.68 ms /    56 tokens (    7.44 ms per token,   134.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.93 ms /    99 runs   (   11.31 ms per token,    88.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1554.84 ms /   155 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 47 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     324.36 ms /    47 tokens (    6.90 ms per token,   144.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.07 ms /    99 runs   (   11.18 ms per token,    89.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1449.49 ms /   146 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 52 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     302.95 ms /    52 tokens (    5.83 ms per token,   171.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1126.51 ms /    99 runs   (   11.38 ms per token,    87.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1447.87 ms /   151 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 45 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     511.25 ms /    45 tokens (   11.36 ms per token,    88.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1148.47 ms /    99 runs   (   11.60 ms per token,    86.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    1678.64 ms /   144 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 58 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     341.41 ms /    58 tokens (    5.89 ms per token,   169.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.43 ms /    99 runs   (   11.65 ms per token,    85.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    1513.53 ms /   157 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 70 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     356.67 ms /    70 tokens (    5.10 ms per token,   196.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1120.70 ms /    99 runs   (   11.32 ms per token,    88.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.81 ms /   169 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 107 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     429.03 ms /   107 tokens (    4.01 ms per token,   249.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.78 ms /    99 runs   (   11.27 ms per token,    88.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    1563.24 ms /   206 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 91 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     429.38 ms /    91 tokens (    4.72 ms per token,   211.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1199.51 ms /    99 runs   (   12.12 ms per token,    82.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    1648.16 ms /   190 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     300.35 ms /    59 tokens (    5.09 ms per token,   196.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1116.35 ms /    99 runs   (   11.28 ms per token,    88.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    1434.98 ms /   158 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 54 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     298.83 ms /    54 tokens (    5.53 ms per token,   180.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1129.73 ms /    99 runs   (   11.41 ms per token,    87.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    1446.95 ms /   153 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     370.17 ms /    82 tokens (    4.51 ms per token,   221.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1114.85 ms /    99 runs   (   11.26 ms per token,    88.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    1503.73 ms /   181 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 89 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     526.78 ms /    89 tokens (    5.92 ms per token,   168.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1146.25 ms /    99 runs   (   11.58 ms per token,    86.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    1691.91 ms /   188 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 40 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     404.64 ms /    40 tokens (   10.12 ms per token,    98.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1142.02 ms /    99 runs   (   11.54 ms per token,    86.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    1565.46 ms /   139 tokens\n",
      "Llama.generate: 23 prefix-match hit, remaining 45 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     314.83 ms /    45 tokens (    7.00 ms per token,   142.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1130.83 ms /    99 runs   (   11.42 ms per token,    87.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    1464.26 ms /   144 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 67 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     360.46 ms /    67 tokens (    5.38 ms per token,   185.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1112.90 ms /    99 runs   (   11.24 ms per token,    88.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    1491.50 ms /   166 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 72 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     383.42 ms /    72 tokens (    5.33 ms per token,   187.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.18 ms /    99 runs   (   11.24 ms per token,    88.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    1515.04 ms /   171 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 76 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     600.84 ms /    76 tokens (    7.91 ms per token,   126.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.99 ms /    99 runs   (   11.18 ms per token,    89.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1726.28 ms /   175 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 100 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     442.46 ms /   100 tokens (    4.42 ms per token,   226.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.37 ms /    99 runs   (   11.65 ms per token,    85.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    1614.34 ms /   199 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 57 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     383.22 ms /    57 tokens (    6.72 ms per token,   148.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.75 ms /    99 runs   (   11.21 ms per token,    89.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1511.07 ms /   156 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 68 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     339.08 ms /    68 tokens (    4.99 ms per token,   200.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.47 ms /    99 runs   (   11.20 ms per token,    89.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1465.84 ms /   167 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 61 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     353.75 ms /    61 tokens (    5.80 ms per token,   172.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.52 ms /    99 runs   (   11.20 ms per token,    89.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1480.65 ms /   160 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     295.27 ms /    46 tokens (    6.42 ms per token,   155.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1106.42 ms /    99 runs   (   11.18 ms per token,    89.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    1419.83 ms /   145 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     355.26 ms /    60 tokens (    5.92 ms per token,   168.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.50 ms /    99 runs   (   11.19 ms per token,    89.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.06 ms /   159 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     315.40 ms /    60 tokens (    5.26 ms per token,   190.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.39 ms /    99 runs   (   11.19 ms per token,    89.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    1440.85 ms /   159 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 53 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     305.52 ms /    53 tokens (    5.76 ms per token,   173.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1031.66 ms /    92 runs   (   11.21 ms per token,    89.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    1353.88 ms /   145 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 58 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     305.07 ms /    58 tokens (    5.26 ms per token,   190.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1133.92 ms /    99 runs   (   11.45 ms per token,    87.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    1457.81 ms /   157 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 53 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     306.81 ms /    53 tokens (    5.79 ms per token,   172.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1118.40 ms /    99 runs   (   11.30 ms per token,    88.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1443.68 ms /   152 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 131 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     486.73 ms /   131 tokens (    3.72 ms per token,   269.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1124.56 ms /    99 runs   (   11.36 ms per token,    88.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    1630.49 ms /   230 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 75 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     374.36 ms /    75 tokens (    4.99 ms per token,   200.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.26 ms /    99 runs   (   11.20 ms per token,    89.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    1502.06 ms /   174 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 93 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     373.09 ms /    93 tokens (    4.01 ms per token,   249.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1131.22 ms /    99 runs   (   11.43 ms per token,    87.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    1523.38 ms /   192 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 53 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     306.44 ms /    53 tokens (    5.78 ms per token,   172.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.61 ms /    99 runs   (   11.23 ms per token,    89.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    1436.29 ms /   152 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     339.32 ms /    51 tokens (    6.65 ms per token,   150.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1119.63 ms /    99 runs   (   11.31 ms per token,    88.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.34 ms /   150 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 55 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     305.71 ms /    55 tokens (    5.56 ms per token,   179.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1108.71 ms /    99 runs   (   11.20 ms per token,    89.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    1432.55 ms /   154 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 44 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     329.03 ms /    44 tokens (    7.48 ms per token,   133.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1195.74 ms /    99 runs   (   12.08 ms per token,    82.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    1543.41 ms /   143 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     389.70 ms /    65 tokens (    6.00 ms per token,   166.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.19 ms /    99 runs   (   11.22 ms per token,    89.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    1519.04 ms /   164 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     348.13 ms /    38 tokens (    9.16 ms per token,   109.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1050.62 ms /    94 runs   (   11.18 ms per token,    89.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    1415.85 ms /   132 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 41 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     325.33 ms /    41 tokens (    7.93 ms per token,   126.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1110.54 ms /    99 runs   (   11.22 ms per token,    89.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    1454.30 ms /   140 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     362.87 ms /    33 tokens (   11.00 ms per token,    90.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1186.87 ms /    99 runs   (   11.99 ms per token,    83.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    1568.27 ms /   132 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 91 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     588.26 ms /    91 tokens (    6.46 ms per token,   154.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1121.67 ms /    99 runs   (   11.33 ms per token,    88.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    1728.48 ms /   190 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 98 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     496.44 ms /    98 tokens (    5.07 ms per token,   197.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.84 ms /    99 runs   (   11.25 ms per token,    88.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1628.50 ms /   197 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 86 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     394.54 ms /    86 tokens (    4.59 ms per token,   217.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1161.62 ms /    99 runs   (   11.73 ms per token,    85.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    1574.84 ms /   185 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     299.55 ms /    46 tokens (    6.51 ms per token,   153.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.03 ms /    99 runs   (   11.18 ms per token,    89.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    1424.92 ms /   145 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 58 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     338.49 ms /    58 tokens (    5.84 ms per token,   171.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1107.16 ms /    99 runs   (   11.18 ms per token,    89.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    1463.80 ms /   157 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 94 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     411.28 ms /    94 tokens (    4.38 ms per token,   228.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1136.21 ms /    99 runs   (   11.48 ms per token,    87.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    1565.98 ms /   193 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 46 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     301.63 ms /    46 tokens (    6.56 ms per token,   152.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.99 ms /    99 runs   (   11.21 ms per token,    89.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    1429.55 ms /   145 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     321.82 ms /    60 tokens (    5.36 ms per token,   186.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1097.27 ms /    94 runs   (   11.67 ms per token,    85.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    1436.57 ms /   154 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 84 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     369.59 ms /    84 tokens (    4.40 ms per token,   227.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1111.43 ms /    99 runs   (   11.23 ms per token,    89.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.49 ms /   183 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 60 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     387.40 ms /    60 tokens (    6.46 ms per token,   154.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1115.91 ms /    99 runs   (   11.27 ms per token,    88.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    1521.51 ms /   159 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     322.22 ms /    50 tokens (    6.44 ms per token,   155.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1109.79 ms /    99 runs   (   11.21 ms per token,    89.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    1450.04 ms /   149 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     315.82 ms /    42 tokens (    7.52 ms per token,   132.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1153.02 ms /    99 runs   (   11.65 ms per token,    85.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    1487.47 ms /   141 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 62 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     327.84 ms /    62 tokens (    5.29 ms per token,   189.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1135.05 ms /    99 runs   (   11.47 ms per token,    87.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.55 ms /   161 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 101 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     415.30 ms /   101 tokens (    4.11 ms per token,   243.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1122.20 ms /    99 runs   (   11.34 ms per token,    88.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    1556.08 ms /   200 tokens\n",
      "Llama.generate: 22 prefix-match hit, remaining 90 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1352.98 ms\n",
      "llama_perf_context_print: prompt eval time =     392.72 ms /    90 tokens (    4.36 ms per token,   229.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1113.81 ms /    99 runs   (   11.25 ms per token,    88.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    1525.13 ms /   189 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time taken: 1.620800597667694\n"
     ]
    }
   ],
   "source": [
    "# get a sense for average time taken\n",
    "\n",
    "import time \n",
    "\n",
    "LIMIT = 100\n",
    "sum = 0\n",
    "for i in range(LIMIT):\n",
    "    print('ITERATION', i)\n",
    "    args['input_text'] = loaded_prompts[i]\n",
    "    start_time = time.time()\n",
    "    cpu_model.get_response(args)\n",
    "    time_taken = time.time() - start_time\n",
    "    sum += time_taken\n",
    "\n",
    "print('Average time taken:', sum / LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
